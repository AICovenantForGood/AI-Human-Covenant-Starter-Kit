# 🧪 AI Risk Assessment – Lite Version (Quick Scan)

**Version:** 1.0  
**Use case:** Early-stage review (prototype, concept, or MVP)  
**Time:** ~10–15 min

This short checklist helps teams quickly identify major ethical, social, and governance risks early in the design process.  
It’s designed to be *fast and conversational*, not exhaustive.

---

## 🧍 Human & Social Impact
- [ ] Could this system harm people or communities if misused?  
- [ ] Might it disadvantage any particular group?  
- [ ] Would I feel comfortable if this affected *me* or someone I care about?

---

## 🔒 Safety & Accountability
- [ ] Is there a clear person or team accountable for the system’s outcomes?  
- [ ] Do we know what happens if the system fails or is misused?  
- [ ] Are we collecting only the data we truly need — and keeping it secure?

---

## ⚖️ Bias & Fairness
- [ ] Have we thought about who might be excluded or treated unfairly?  
- [ ] Are we testing outputs on diverse examples?

---

## 🔍 Transparency & Trust
- [ ] Will users know they’re interacting with AI?  
- [ ] Can we explain how this system works at a high level?  
- [ ] Would we be comfortable showing its logic to the public?

---

## 🌍 Long-Term Considerations
- [ ] Could this scale in a way that causes harm or amplifies risks?  
- [ ] Is the environmental cost reasonable and measured?  
- [ ] Are we aligned with broader principles like the **AI-Human Covenant**?

---

✅ **Tip:** If you answer “no” or “not sure” to more than 2 questions, it’s worth a deeper review before moving forward.

---

*This lite checklist is a starting point — not a substitute for comprehensive risk assessment.*
